{
    "docs": [
        {
            "location": "/",
            "text": "scikit-rebate\n is a scikit-learn-compatible Python implementation of ReBATE, a suite of \nRelief\n-based feature selection algorithms for Machine Learning. As of 5/7/18, \nthis project is still under active development\n and we encourage you to check back on this repository regularly for updates.\n\n\nThese algorithms excel at identifying features that are predictive of the outcome in supervised learning problems, and are especially good at identifying feature interactions that are normally overlooked by standard feature selection methods.\n\n\nThe main benefit of Relief-based algorithms is that they identify feature interactions without having to exhaustively check every pairwise interaction, thus taking significantly less time than exhaustive pairwise search.\n\n\nRelief-based algorithms are commonly applied to genetic analyses, where epistasis (i.e., feature interactions) is common. However, the algorithms implemented in this package can be applied to almost any supervised classification data set and supports:\n\n\n\n\n\n\nA mix of categorical and/or continuous features\n\n\n\n\n\n\nData with missing values\n\n\n\n\n\n\nBinary endpoints (i.e., classification)\n\n\n\n\n\n\nMulti-class endpoints (i.e., classification)\n\n\n\n\n\n\nContinuous endpoints (i.e., regression)",
            "title": "Home"
        },
        {
            "location": "/installing/",
            "text": "scikit-rebate is built on top of the following existing Python packages:\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\nSciPy\n\n\n\n\n\n\nscikit-learn\n\n\n\n\n\n\nAll of the necessary Python packages can be installed via the \nAnaconda Python distribution\n, which we strongly recommend that you use. We also strongly recommend that you use Python 3 over Python 2 if you're given the choice.\n\n\nNumPy, SciPy, and scikit-learn can be installed in Anaconda via the command:\n\n\nconda install numpy scipy scikit-learn\n\n\n\n\nOnce the prerequisites are installed, you should be able to install scikit-rebate with a pip command:\n\n\npip install skrebate\n\n\n\n\nYou can retrieve basic information about your installed version of skrebate with the following pip command:\n\n\npip show skrebate\n\n\n\n\nYou can check that you have the most up to date pypi release of skrebate with the following pip command:\n\n\npip install skrebate -U\n\n\n\n\nPlease \nfile a new issue\n if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/using/",
            "text": "We have designed the Relief algorithms to be integrated directly into scikit-learn machine learning workflows. Below, we provide code samples showing how the various Relief algorithms can be used as feature selection methods in scikit-learn pipelines.\n\n\nFor details on the algorithmic differences between the various Relief algorithms, please refer to \nthis research paper\n.\n\n\nReliefF\n\n\nReliefF is the most basic of the Relief-based feature selection algorithms, and it requires you to specify the number of nearest neighbors to consider in the scoring algorithm. The parameters for the ReliefF algorithm are as follows:\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\nn_features_to_select\n\n\nAny positive integer or float\n\n\nThe number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the ReliefF scoring process.\n\n\n\n\n\n\nn_neighbors\n\n\nAny positive integer\n\n\nThe number of neighbors to consider when assigning feature importance scores. If a float number is provided, that percentage of training samples is used as the number of neighbors. More neighbors results in more accurate scores, but takes longer.\n\n\n\n\n\n\ndiscrete_limit\n\n\nAny positive integer\n\n\nValue used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.\n\n\n\n\n\n\nn_jobs\n\n\nAny positive integer or -1\n\n\nThe number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nSURF\n\n\nSURF, SURF*, and MultiSURF are all extensions to the ReliefF algorithm that automatically determine the ideal number of neighbors to consider when scoring the features.\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\nn_features_to_select\n\n\nAny positive integer\n\n\nThe number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the SURF scoring process.\n\n\n\n\n\n\ndiscrete_limit\n\n\nAny positive integer\n\n\nValue used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.\n\n\n\n\n\n\nn_jobs\n\n\nAny positive integer or -1\n\n\nThe number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import SURF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(SURF(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nSURF*\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\nn_features_to_select\n\n\nAny positive integer\n\n\nThe number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the SURF* scoring process.\n\n\n\n\n\n\ndiscrete_limit\n\n\nAny positive integer\n\n\nValue used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.\n\n\n\n\n\n\nn_jobs\n\n\nAny positive integer or -1\n\n\nThe number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import SURFstar\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(SURFstar(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nMultiSURF\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\nn_features_to_select\n\n\nAny positive integer\n\n\nThe number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the MultiSURF scoring process.\n\n\n\n\n\n\ndiscrete_limit\n\n\nAny positive integer\n\n\nValue used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.\n\n\n\n\n\n\nn_jobs\n\n\nAny positive integer or -1\n\n\nThe number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import MultiSURF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(MultiSURF(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nMultiSURF*\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\nn_features_to_select\n\n\nAny positive integer\n\n\nThe number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the MultiSURF* scoring process.\n\n\n\n\n\n\ndiscrete_limit\n\n\nAny positive integer\n\n\nValue used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.\n\n\n\n\n\n\nn_jobs\n\n\nAny positive integer or -1\n\n\nThe number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import MultiSURFstar\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(MultiSURFstar(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nTuRF\n\n\nTURF advances the feature selection process from a single round to a multi-round process, and can be used in conjunction with any of the Relief-based algorithms. TURF begins with all of the features in the first round, scores them using one of the Relief-based algorithms, then eliminates a portion of them that have the worst scores. With this reduced feature set, TURF again scores the remaining features and eliminates a portion of the worst-scoring features. This process is repeated until a predefined number of features remain or some maximum number of iterations have completed. Presently, there are two ways to run the 'TuRF' iterative feature selection wrapper around any of the given core Relief-based algorithm in scikit-rebate. First, there is a custom TuRF implementation, hard coded into scikit-rebate designed to operate in the same way as specified in the original TuRF paper.  The second, uses the \nRecursive Feature Elimination\n, as \nimplemented\n in scikit-learn. These approaches are similar but not equivalent. We recommend using built in scikit-rebate TuRF. Examples for running TuRF using either approach are given below.\n\n\nTuRF implemented in scikit-rebate\n\n\nWith this TuRF implementation, the (pct) parameter inversely determines the number of TuRF scoring iterations (i.e 1/pct), and pct also determines the percent of features eliminated from scoring each iteration. The n_features_to_select parameter simply determines the number of top scoring features to pass onto the pipeline. This TuRF approach should be used to most closely follow the original TuRF description, as well as to be able to obtain individual feature scores following the completion of TuRF. This method also keeps information about when features were dropped from consideration during progressive TuRF iterations. It does this by assigning 'removed' features a token score that simply indicates which iteration the feature was removed from scoring. All features removed from scoring during TuRF will be assigned a score lower than the lowest feature score in the final feature set.  All features removed at the same time are assigned the same discounted token feature score. This is particularly important when accessing the feature scores as described later. For an example of how to use scikit-rebate TuRF in a scikit learn pipeline see below.\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import TuRF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\nheaders = list(genetic_data.drop(\"class\", axis=1))\n\nclf = make_pipeline(TuRF(core_algorithm=\"ReliefF\", n_features_to_select=2, pct=0.5),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels, fit_params={'turf__headers': headers})))\n>>> 0.795\n\n\n\n\nTuRF via RFE\n\n\nWith this strategy for running TuRF the main difference is that the number of TuRF iterations is not controlled by the pct parameter, rather, iterations run until the specified number of n_features_to_select have been reached. Each iteration, the 'step' parameter controls the number of features removed, either as a percent between 0 and 1 or an integer count of features to remove each iteration. One critical shortcoming of this approach is that there is no way to obtain the individual feature scores when using RFE to do 'TuRF' scoring. See \nRecursive Feature Elimination\n, for more details.\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(RFE(ReliefF(), n_features_to_select=2, step = 0.5),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795\n\n\n\n\nAcquiring feature importance scores\n\n\nIn many cases, it may be useful to compute feature importance scores without actually performing feature selection. We have made it possible to access all Relief-based algorithm's scores via the \nfeature_importances_\n attribute. Below are code examples showing how to access the scores from the any core Relief-based algorithm as well as from TuRF in combination with a Relief-based algorithm. The first example illustrates how scores may be obtained from ReliefF, adding a split of the loaded data into training and testing since we are not running ReliefF as part of a scikit pipeline like above.  \n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.model_selection import train_test_split\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\n# Make sure to compute the feature importance scores from only your training set\nX_train, X_test, y_train, y_test = train_test_split(features, labels)\n\nfs = ReliefF()\nfs.fit(X_train, y_train)\n\nfor feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns,\n                                       fs.feature_importances_):\n    print(feature_name, '\\t', feature_score)\n\n>>>N0    -0.0000166666666667\n>>>N1    -0.006175\n>>>N2    -0.0079\n>>>N3    -0.006275\n>>>N4    -0.00684166666667\n>>>N5    -0.0104416666667\n>>>N6    -0.010275\n>>>N7    -0.00785\n>>>N8    -0.00824166666667\n>>>N9    -0.00515\n>>>N10   -0.000216666666667\n>>>N11   -0.0039\n>>>N12   -0.00291666666667\n>>>N13   -0.00345833333333\n>>>N14   -0.00324166666667\n>>>N15   -0.00886666666667\n>>>N16   -0.00611666666667\n>>>N17   -0.007325\n>>>P1    0.108966666667\n>>>P2    0.111\n\n\n\n\nIn this second example we show how to obtain scores when using ReliefF in combination with TuRF (some slight differences). In this example we differently assume that the loaded dataset is the training dataset and we do not need to split the data into training and testing prior to running ReliefF. The main difference here is that when using TuRF, fs.fit also requires 'headers' as an argument.  \n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate.turf import TuRF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\nheaders = list(genetic_data.drop(\"class\", axis=1))\nfs = TuRF(core_algorithm=\"ReliefF\", n_features_to_select=2, pct=0.5,verbose=True)\nfs.fit(features, labels, headers)\nfor feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns, fs.feature_importances_):\n    print(feature_name, '\\t', feature_score)\n\n>>>N0    -0.00103125\n>>>N1    -0.0107515625\n>>>N2    -0.012890625\n>>>N3    -0.012890625\n>>>N4    -0.012890625\n>>>N5    -0.012890625\n>>>N6    -0.012890625\n>>>N7    -0.012890625\n>>>N8    -0.0107515625\n>>>N9    -0.012890625\n>>>N10   -0.00118125\n>>>N11   -0.012890625\n>>>N12   -0.0107515625\n>>>N13   -0.0086125\n>>>N14   -0.0107515625\n>>>N15   -0.012890625\n>>>N16   -0.0107515625\n>>>N17   -0.012890625\n>>>P1    0.20529375\n>>>P2    0.17374375\n\n\n\n\nTo retrieve an np.array of feature importance scores in the original dataset ordering then add the following... (this also works with any core algorithm)\n\n\nprint(fs.feature\\_importances\\_)\n>>>[-0.00103125 -0.01075156 -0.01289062 -0.01289062 -0.01289062 -0.01289062\n -0.01289062 -0.01289062 -0.01075156 -0.01289062 -0.00118125 -0.01289062\n -0.01075156 -0.0086125  -0.01075156 -0.01289062 -0.01075156 -0.01289062\n  0.20529375  0.17374375]\n\n\n\n\nTo retrieve a list of indices for the top scoring features ranked by increasing score, then add the following... (this also works with any core algorithm)\n\n\nprint(fs.top\\_features_)\n>>>[13, 0, 10, 19, 18]\n\n\n\n\nTo sort features by decreasing score along with their names, and simultaneously indicate which features have been assigned a token TuRF feature score (since they were removed from consideration at some point) then add the following...\n\n\nscored\\_features = len(fs.top\\_features_)\nsorted_names = sorted(scoreDict, key=lambda x: scoreDict[x], reverse=True)\nn = 1\nfor k in sorted\\_names:\n    if n < scored\\_features +1 :\n        print(k, '\\t', scoreDict[k],'\\t',n) \n    else:\n        print(k, '\\t', scoreDict[k],'\\t','*') \n    n += 1\n\n>>>P1    0.20529375      1\n>>>P2    0.17374375      2\n>>>N0    -0.00103125     3\n>>>N10   -0.00118125     4\n>>>N13   -0.0086125      5\n>>>N1    -0.0107515625   *\n>>>N14   -0.0107515625   *\n>>>N16   -0.0107515625   *\n>>>N8    -0.0107515625   *\n>>>N12   -0.0107515625   *\n>>>N3    -0.012890625    *\n>>>N2    -0.012890625    *\n>>>N7    -0.012890625    *\n>>>N17   -0.012890625    *\n>>>N5    -0.012890625    *\n>>>N15   -0.012890625    *\n>>>N11   -0.012890625    *\n>>>N4    -0.012890625    *\n>>>N9    -0.012890625    *\n>>>N6    -0.012890625    *\n\n\n\n\nLastly, to output these scores to a text file in a format similar to how it is done in our alternative implementation of stand alone \nReBATE\n, add something like the following...\n\n\nalgorithm = 'TuRF_ReliefF'\ndiscreteLimit = '10'\nnumNeighbors = '100'\noutfile = algorithm + '-scores-' + discreteLimit + '-' + numNeighbors + '-' + 'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.txt'\nfh = open(outfile, 'w')\nfh.write(algorithm + ' Analysis Completed with REBATE\\n')\nfh.write('Run Time (sec): ' + str('NA') + '\\n')\nfh.write('=== SCORES ===\\n')\nn = 1\nfor k in sorted_names:\n    if n < scored_features +1 :\n        fh.write(str(k) + '\\t' + str(scoreDict[k])  + '\\t' + str(n) +'\\n')\n    else:\n        fh.write(str(k) + '\\t' + str(scoreDict[k])  + '\\t' + '*' +'\\n')\n    n+=1\nfh.close()\n\n\n\n\nThis ordered list and text output can be achieved similarly for any core Relief-based algorithm by just removing the 'if n < scored_features +1 :' loop and the else statement adding the '*'. \n\n\nGeneral Usage Guidelines\n\n\n1.) When performing feature selection, there is no universally best way to determine where to draw the cuttoff for including features. When using original Relief or ReliefF it has been suggested that features yielding a negative value score, can be confidently filtered out. This guideline is believed to be extendable to SURF, SURF*, MultiSURF*, and MultiSURF, however please note that features with a negative score are not necessarily irrelevant, and those with a positive score are not necessarily relevant. Instead, scores are most effectively interpreted as the relative evidence that a given feature is predictive of outcome. Thus, while it may be reasonable to only filter out features with a negative score, in practice it may be more useful to select some 'top' number of features to pass onto modeling. \n\n\n2.) In very large feature spaces users can expect core Relief-based algorithm scores to become less reliable when run on their own. This is because as the feature space becomes very large, the determination of nearest neighbors becomes more random.  As a result, in very large feature spaces (e.g. > 10,000 features), users should consider combining a core Relief-based algorithm with an iterative approach such as TuRF (implemented here) or VLSRelieF, or Iterative Relief. \n\n\n3.) When scaling up to big data problems, keep in mind that the data aspect that slows down ReBATE methods the most is the number of training instances, since Relief algorithms scale linearly with the number of features, but quadratically with the number of training instances. This is is the result of Relief-based methods needing to calculate a distance array (i.e. all pairwise distances between instances in the training dataset).  If you have a very large number of training instances available, consider utilizing a class balanced random sampling of that dataset when running any ReBATE methods to save on memory and computational time.",
            "title": "Using skrebate"
        },
        {
            "location": "/using/#relieff",
            "text": "ReliefF is the most basic of the Relief-based feature selection algorithms, and it requires you to specify the number of nearest neighbors to consider in the scoring algorithm. The parameters for the ReliefF algorithm are as follows:    Parameter  Valid values  Effect    n_features_to_select  Any positive integer or float  The number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the ReliefF scoring process.    n_neighbors  Any positive integer  The number of neighbors to consider when assigning feature importance scores. If a float number is provided, that percentage of training samples is used as the number of neighbors. More neighbors results in more accurate scores, but takes longer.    discrete_limit  Any positive integer  Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.    n_jobs  Any positive integer or -1  The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "ReliefF"
        },
        {
            "location": "/using/#surf",
            "text": "SURF, SURF*, and MultiSURF are all extensions to the ReliefF algorithm that automatically determine the ideal number of neighbors to consider when scoring the features.    Parameter  Valid values  Effect    n_features_to_select  Any positive integer  The number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the SURF scoring process.    discrete_limit  Any positive integer  Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.    n_jobs  Any positive integer or -1  The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import SURF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(SURF(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "SURF"
        },
        {
            "location": "/using/#surf_1",
            "text": "Parameter  Valid values  Effect    n_features_to_select  Any positive integer  The number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the SURF* scoring process.    discrete_limit  Any positive integer  Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.    n_jobs  Any positive integer or -1  The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import SURFstar\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(SURFstar(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "SURF*"
        },
        {
            "location": "/using/#multisurf",
            "text": "Parameter  Valid values  Effect    n_features_to_select  Any positive integer  The number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the MultiSURF scoring process.    discrete_limit  Any positive integer  Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.    n_jobs  Any positive integer or -1  The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import MultiSURF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(MultiSURF(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "MultiSURF"
        },
        {
            "location": "/using/#multisurf_1",
            "text": "Parameter  Valid values  Effect    n_features_to_select  Any positive integer  The number of best features to retain after the feature selection process. The \"best\" features are the highest-scored features according to the MultiSURF* scoring process.    discrete_limit  Any positive integer  Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.    n_jobs  Any positive integer or -1  The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import MultiSURFstar\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(MultiSURFstar(n_features_to_select=2),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "MultiSURF*"
        },
        {
            "location": "/using/#turf",
            "text": "TURF advances the feature selection process from a single round to a multi-round process, and can be used in conjunction with any of the Relief-based algorithms. TURF begins with all of the features in the first round, scores them using one of the Relief-based algorithms, then eliminates a portion of them that have the worst scores. With this reduced feature set, TURF again scores the remaining features and eliminates a portion of the worst-scoring features. This process is repeated until a predefined number of features remain or some maximum number of iterations have completed. Presently, there are two ways to run the 'TuRF' iterative feature selection wrapper around any of the given core Relief-based algorithm in scikit-rebate. First, there is a custom TuRF implementation, hard coded into scikit-rebate designed to operate in the same way as specified in the original TuRF paper.  The second, uses the  Recursive Feature Elimination , as  implemented  in scikit-learn. These approaches are similar but not equivalent. We recommend using built in scikit-rebate TuRF. Examples for running TuRF using either approach are given below.",
            "title": "TuRF"
        },
        {
            "location": "/using/#turf-implemented-in-scikit-rebate",
            "text": "With this TuRF implementation, the (pct) parameter inversely determines the number of TuRF scoring iterations (i.e 1/pct), and pct also determines the percent of features eliminated from scoring each iteration. The n_features_to_select parameter simply determines the number of top scoring features to pass onto the pipeline. This TuRF approach should be used to most closely follow the original TuRF description, as well as to be able to obtain individual feature scores following the completion of TuRF. This method also keeps information about when features were dropped from consideration during progressive TuRF iterations. It does this by assigning 'removed' features a token score that simply indicates which iteration the feature was removed from scoring. All features removed from scoring during TuRF will be assigned a score lower than the lowest feature score in the final feature set.  All features removed at the same time are assigned the same discounted token feature score. This is particularly important when accessing the feature scores as described later. For an example of how to use scikit-rebate TuRF in a scikit learn pipeline see below.  import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import TuRF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\nheaders = list(genetic_data.drop(\"class\", axis=1))\n\nclf = make_pipeline(TuRF(core_algorithm=\"ReliefF\", n_features_to_select=2, pct=0.5),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels, fit_params={'turf__headers': headers})))\n>>> 0.795",
            "title": "TuRF implemented in scikit-rebate"
        },
        {
            "location": "/using/#turf-via-rfe",
            "text": "With this strategy for running TuRF the main difference is that the number of TuRF iterations is not controlled by the pct parameter, rather, iterations run until the specified number of n_features_to_select have been reached. Each iteration, the 'step' parameter controls the number of features removed, either as a percent between 0 and 1 or an integer count of features to remove each iteration. One critical shortcoming of this approach is that there is no way to obtain the individual feature scores when using RFE to do 'TuRF' scoring. See  Recursive Feature Elimination , for more details.  import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\nclf = make_pipeline(RFE(ReliefF(), n_features_to_select=2, step = 0.5),\n                    RandomForestClassifier(n_estimators=100))\n\nprint(np.mean(cross_val_score(clf, features, labels)))\n>>> 0.795",
            "title": "TuRF via RFE"
        },
        {
            "location": "/using/#acquiring-feature-importance-scores",
            "text": "In many cases, it may be useful to compute feature importance scores without actually performing feature selection. We have made it possible to access all Relief-based algorithm's scores via the  feature_importances_  attribute. Below are code examples showing how to access the scores from the any core Relief-based algorithm as well as from TuRF in combination with a Relief-based algorithm. The first example illustrates how scores may be obtained from ReliefF, adding a split of the loaded data into training and testing since we are not running ReliefF as part of a scikit pipeline like above.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate import ReliefF\nfrom sklearn.model_selection import train_test_split\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\n\n# Make sure to compute the feature importance scores from only your training set\nX_train, X_test, y_train, y_test = train_test_split(features, labels)\n\nfs = ReliefF()\nfs.fit(X_train, y_train)\n\nfor feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns,\n                                       fs.feature_importances_):\n    print(feature_name, '\\t', feature_score)\n\n>>>N0    -0.0000166666666667\n>>>N1    -0.006175\n>>>N2    -0.0079\n>>>N3    -0.006275\n>>>N4    -0.00684166666667\n>>>N5    -0.0104416666667\n>>>N6    -0.010275\n>>>N7    -0.00785\n>>>N8    -0.00824166666667\n>>>N9    -0.00515\n>>>N10   -0.000216666666667\n>>>N11   -0.0039\n>>>N12   -0.00291666666667\n>>>N13   -0.00345833333333\n>>>N14   -0.00324166666667\n>>>N15   -0.00886666666667\n>>>N16   -0.00611666666667\n>>>N17   -0.007325\n>>>P1    0.108966666667\n>>>P2    0.111  In this second example we show how to obtain scores when using ReliefF in combination with TuRF (some slight differences). In this example we differently assume that the loaded dataset is the training dataset and we do not need to split the data into training and testing prior to running ReliefF. The main difference here is that when using TuRF, fs.fit also requires 'headers' as an argument.    import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom skrebate.turf import TuRF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ngenetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n                           sep='\\t', compression='gzip')\n\nfeatures, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values\nheaders = list(genetic_data.drop(\"class\", axis=1))\nfs = TuRF(core_algorithm=\"ReliefF\", n_features_to_select=2, pct=0.5,verbose=True)\nfs.fit(features, labels, headers)\nfor feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns, fs.feature_importances_):\n    print(feature_name, '\\t', feature_score)\n\n>>>N0    -0.00103125\n>>>N1    -0.0107515625\n>>>N2    -0.012890625\n>>>N3    -0.012890625\n>>>N4    -0.012890625\n>>>N5    -0.012890625\n>>>N6    -0.012890625\n>>>N7    -0.012890625\n>>>N8    -0.0107515625\n>>>N9    -0.012890625\n>>>N10   -0.00118125\n>>>N11   -0.012890625\n>>>N12   -0.0107515625\n>>>N13   -0.0086125\n>>>N14   -0.0107515625\n>>>N15   -0.012890625\n>>>N16   -0.0107515625\n>>>N17   -0.012890625\n>>>P1    0.20529375\n>>>P2    0.17374375  To retrieve an np.array of feature importance scores in the original dataset ordering then add the following... (this also works with any core algorithm)  print(fs.feature\\_importances\\_)\n>>>[-0.00103125 -0.01075156 -0.01289062 -0.01289062 -0.01289062 -0.01289062\n -0.01289062 -0.01289062 -0.01075156 -0.01289062 -0.00118125 -0.01289062\n -0.01075156 -0.0086125  -0.01075156 -0.01289062 -0.01075156 -0.01289062\n  0.20529375  0.17374375]  To retrieve a list of indices for the top scoring features ranked by increasing score, then add the following... (this also works with any core algorithm)  print(fs.top\\_features_)\n>>>[13, 0, 10, 19, 18]  To sort features by decreasing score along with their names, and simultaneously indicate which features have been assigned a token TuRF feature score (since they were removed from consideration at some point) then add the following...  scored\\_features = len(fs.top\\_features_)\nsorted_names = sorted(scoreDict, key=lambda x: scoreDict[x], reverse=True)\nn = 1\nfor k in sorted\\_names:\n    if n < scored\\_features +1 :\n        print(k, '\\t', scoreDict[k],'\\t',n) \n    else:\n        print(k, '\\t', scoreDict[k],'\\t','*') \n    n += 1\n\n>>>P1    0.20529375      1\n>>>P2    0.17374375      2\n>>>N0    -0.00103125     3\n>>>N10   -0.00118125     4\n>>>N13   -0.0086125      5\n>>>N1    -0.0107515625   *\n>>>N14   -0.0107515625   *\n>>>N16   -0.0107515625   *\n>>>N8    -0.0107515625   *\n>>>N12   -0.0107515625   *\n>>>N3    -0.012890625    *\n>>>N2    -0.012890625    *\n>>>N7    -0.012890625    *\n>>>N17   -0.012890625    *\n>>>N5    -0.012890625    *\n>>>N15   -0.012890625    *\n>>>N11   -0.012890625    *\n>>>N4    -0.012890625    *\n>>>N9    -0.012890625    *\n>>>N6    -0.012890625    *  Lastly, to output these scores to a text file in a format similar to how it is done in our alternative implementation of stand alone  ReBATE , add something like the following...  algorithm = 'TuRF_ReliefF'\ndiscreteLimit = '10'\nnumNeighbors = '100'\noutfile = algorithm + '-scores-' + discreteLimit + '-' + numNeighbors + '-' + 'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.txt'\nfh = open(outfile, 'w')\nfh.write(algorithm + ' Analysis Completed with REBATE\\n')\nfh.write('Run Time (sec): ' + str('NA') + '\\n')\nfh.write('=== SCORES ===\\n')\nn = 1\nfor k in sorted_names:\n    if n < scored_features +1 :\n        fh.write(str(k) + '\\t' + str(scoreDict[k])  + '\\t' + str(n) +'\\n')\n    else:\n        fh.write(str(k) + '\\t' + str(scoreDict[k])  + '\\t' + '*' +'\\n')\n    n+=1\nfh.close()  This ordered list and text output can be achieved similarly for any core Relief-based algorithm by just removing the 'if n < scored_features +1 :' loop and the else statement adding the '*'.",
            "title": "Acquiring feature importance scores"
        },
        {
            "location": "/using/#general-usage-guidelines",
            "text": "1.) When performing feature selection, there is no universally best way to determine where to draw the cuttoff for including features. When using original Relief or ReliefF it has been suggested that features yielding a negative value score, can be confidently filtered out. This guideline is believed to be extendable to SURF, SURF*, MultiSURF*, and MultiSURF, however please note that features with a negative score are not necessarily irrelevant, and those with a positive score are not necessarily relevant. Instead, scores are most effectively interpreted as the relative evidence that a given feature is predictive of outcome. Thus, while it may be reasonable to only filter out features with a negative score, in practice it may be more useful to select some 'top' number of features to pass onto modeling.   2.) In very large feature spaces users can expect core Relief-based algorithm scores to become less reliable when run on their own. This is because as the feature space becomes very large, the determination of nearest neighbors becomes more random.  As a result, in very large feature spaces (e.g. > 10,000 features), users should consider combining a core Relief-based algorithm with an iterative approach such as TuRF (implemented here) or VLSRelieF, or Iterative Relief.   3.) When scaling up to big data problems, keep in mind that the data aspect that slows down ReBATE methods the most is the number of training instances, since Relief algorithms scale linearly with the number of features, but quadratically with the number of training instances. This is is the result of Relief-based methods needing to calculate a distance array (i.e. all pairwise distances between instances in the training dataset).  If you have a very large number of training instances available, consider utilizing a class balanced random sampling of that dataset when running any ReBATE methods to save on memory and computational time.",
            "title": "General Usage Guidelines"
        },
        {
            "location": "/contributing/",
            "text": "We welcome you to \ncheck the existing issues\n for bugs or enhancements to work on. If you have an idea for an extension to scikit-rebate, please \nfile a new issue\n so we can discuss it.\n\n\nProject layout\n\n\nThe latest stable release of scikit-rebate is on the \nmaster branch\n, whereas the latest version of scikit-rebate in development is on the \ndevelopment branch\n. Make sure you are looking at and working on the correct branch if you're looking to contribute code.\n\n\nIn terms of directory structure:\n\n\n\n\nAll of scikit-rebate's code sources are in the \nskrebate\n directory\n\n\nThe documentation sources are in the \ndocs_sources\n directory\n\n\nThe latest documentation build is in the \ndocs\n directory\n\n\nUnit tests for scikit-rebate are in the \ntests.py\n file\n\n\n\n\nMake sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the \ndevelopment\n branch.\n\n\nHow to contribute\n\n\nThe preferred way to contribute to scikit-rebate is to fork the \n\nmain repository\n on\nGitHub:\n\n\n\n\n\n\nFork the \nproject repository\n:\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.\n\n\n\n\n\n\nClone this copy to your local disk:\n\n\n  $ git clone git@github.com:YourLogin/scikit-rebate.git\n  $ cd scikit-rebate\n\n\n\n\n\n\n\nCreate a branch to hold your changes:\n\n\n  $ git checkout -b my-contribution\n\n\n\n\n\n\n\nMake sure your local environment is setup correctly for development. Installation instructions are almost identical to \nthe user instructions\n except that scikit-rebate should \nnot\n be installed. If you have scikit-rebate installed on your computer, then make sure you are using a virtual environment that does not have scikit-rebate installed. Furthermore, you should make sure you have installed the \nnose\n package into your development environment so that you can test changes locally.\n\n\n  $ conda install nose\n\n\n\n\n\n\n\nStart making changes on your newly created branch, remembering to never work on the \nmaster\n branch! Work on this copy on your computer using Git to do the version control.\n\n\n\n\n\n\nOnce some changes are saved locally, you can use your tweaked version of scikit-rebate by navigating to the project's base directory and running scikit-rebate in a script. \n\n\n\n\n\n\nTo check your changes haven't broken any existing tests and to check new tests you've added pass run the following (note, you must have the \nnose\n package installed within your dev environment for this to work):\n\n\n  $ nosetests -s -v\n\n\n\n\n\n\n\nWhen you're done editing and local testing, run:\n\n\n  $ git add modified_files\n  $ git commit\n\n\n\n\n\n\n\nto record your changes in Git, then push them to GitHub with:\n\n\n      $ git push -u origin my-contribution\n\n\n\nFinally, go to the web page of your fork of the scikit-rebate repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the \ndevelopment\n branch, as the \nmaster\n branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.\n\n\n(For details on the above look up the \nGit documentation\n on the web.)\n\n\nBefore submitting your pull request\n\n\nBefore you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.\n\n\nIf your contribution changes scikit-rebate in any way:\n\n\n\n\n\n\nUpdate the \ndocumentation\n so all of your changes are reflected there.\n\n\n\n\n\n\nUpdate the \nREADME\n if anything there has changed.\n\n\n\n\n\n\nIf your contribution involves any code changes:\n\n\n\n\n\n\nUpdate the \nproject unit tests\n to test your code changes.\n\n\n\n\n\n\nMake sure that your code is properly commented with \ndocstrings\n and comments explaining your rationale behind non-obvious coding practices.\n\n\n\n\n\n\nIf your contribution requires a new library dependency:\n\n\n\n\n\n\nDouble-check that the new dependency is easy to install via \npip\n or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep scikit-rebate easy to install.\n\n\n\n\n\n\nAdd a line to pip install the library to \n.travis_install.sh\n\n\n\n\n\n\nAdd a line to print the version of the library to \n.travis_install.sh\n\n\n\n\n\n\nSimilarly add a line to print the version of the library to \n.travis_test.sh\n\n\n\n\n\n\nUpdating the documentation\n\n\nWe use \nmkdocs\n to manage our \ndocumentation\n. This allows us to write the docs in Markdown and compile them to HTML as needed. Below are a few useful commands to know when updating the documentation. Make sure that you are running them in the base repository directory.\n\n\n\n\n\n\nmkdocs serve\n: Hosts of a local version of the documentation that you can access at the provided URL. The local version will update automatically as you save changes to the documentation.\n\n\n\n\n\n\nmkdocs build --clean\n: Creates a fresh build of the documentation in HTML. Always run this before deploying the documentation to GitHub.\n\n\n\n\n\n\nmkdocs gh-deploy\n: Deploys the documentation to GitHub. If you're deploying on your fork of scikit-rebate, the online documentation should be accessible at \nhttp://<YOUR GITHUB USERNAME>.github.io/scikit-rebate/\n. Generally, you shouldn't need to run this command because you can view your changes with \nmkdocs serve\n.\n\n\n\n\n\n\nAfter submitting your pull request\n\n\nAfter submitting your pull request, \nTravis-CI\n will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.\n\n\nCheck back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#project-layout",
            "text": "The latest stable release of scikit-rebate is on the  master branch , whereas the latest version of scikit-rebate in development is on the  development branch . Make sure you are looking at and working on the correct branch if you're looking to contribute code.  In terms of directory structure:   All of scikit-rebate's code sources are in the  skrebate  directory  The documentation sources are in the  docs_sources  directory  The latest documentation build is in the  docs  directory  Unit tests for scikit-rebate are in the  tests.py  file   Make sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the  development  branch.",
            "title": "Project layout"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "The preferred way to contribute to scikit-rebate is to fork the  main repository  on\nGitHub:    Fork the  project repository :\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.    Clone this copy to your local disk:    $ git clone git@github.com:YourLogin/scikit-rebate.git\n  $ cd scikit-rebate    Create a branch to hold your changes:    $ git checkout -b my-contribution    Make sure your local environment is setup correctly for development. Installation instructions are almost identical to  the user instructions  except that scikit-rebate should  not  be installed. If you have scikit-rebate installed on your computer, then make sure you are using a virtual environment that does not have scikit-rebate installed. Furthermore, you should make sure you have installed the  nose  package into your development environment so that you can test changes locally.    $ conda install nose    Start making changes on your newly created branch, remembering to never work on the  master  branch! Work on this copy on your computer using Git to do the version control.    Once some changes are saved locally, you can use your tweaked version of scikit-rebate by navigating to the project's base directory and running scikit-rebate in a script.     To check your changes haven't broken any existing tests and to check new tests you've added pass run the following (note, you must have the  nose  package installed within your dev environment for this to work):    $ nosetests -s -v    When you're done editing and local testing, run:    $ git add modified_files\n  $ git commit    to record your changes in Git, then push them to GitHub with:        $ git push -u origin my-contribution  Finally, go to the web page of your fork of the scikit-rebate repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the  development  branch, as the  master  branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.  (For details on the above look up the  Git documentation  on the web.)",
            "title": "How to contribute"
        },
        {
            "location": "/contributing/#before-submitting-your-pull-request",
            "text": "Before you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.  If your contribution changes scikit-rebate in any way:    Update the  documentation  so all of your changes are reflected there.    Update the  README  if anything there has changed.    If your contribution involves any code changes:    Update the  project unit tests  to test your code changes.    Make sure that your code is properly commented with  docstrings  and comments explaining your rationale behind non-obvious coding practices.    If your contribution requires a new library dependency:    Double-check that the new dependency is easy to install via  pip  or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep scikit-rebate easy to install.    Add a line to pip install the library to  .travis_install.sh    Add a line to print the version of the library to  .travis_install.sh    Similarly add a line to print the version of the library to  .travis_test.sh",
            "title": "Before submitting your pull request"
        },
        {
            "location": "/contributing/#updating-the-documentation",
            "text": "We use  mkdocs  to manage our  documentation . This allows us to write the docs in Markdown and compile them to HTML as needed. Below are a few useful commands to know when updating the documentation. Make sure that you are running them in the base repository directory.    mkdocs serve : Hosts of a local version of the documentation that you can access at the provided URL. The local version will update automatically as you save changes to the documentation.    mkdocs build --clean : Creates a fresh build of the documentation in HTML. Always run this before deploying the documentation to GitHub.    mkdocs gh-deploy : Deploys the documentation to GitHub. If you're deploying on your fork of scikit-rebate, the online documentation should be accessible at  http://<YOUR GITHUB USERNAME>.github.io/scikit-rebate/ . Generally, you shouldn't need to run this command because you can view your changes with  mkdocs serve .",
            "title": "Updating the documentation"
        },
        {
            "location": "/contributing/#after-submitting-your-pull-request",
            "text": "After submitting your pull request,  Travis-CI  will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.  Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "After submitting your pull request"
        },
        {
            "location": "/releases/",
            "text": "scikit-rebate 0.6\n\n\n\n\n\n\nFixed internal TuRF implementation so that it outputs scores for all features. Those that make it to the last iteration get true core algorithm scoring, while those that were removed along the way are assigned token scores (lower than the lowest true scoring feature) that indicate when the respective feature(s) were removed. This also alows for greater flexibility in the user specifying the number for features to return. \n\n\n\n\n\n\nUpdated the usage documentation to demonstrate how to use RFE as well as the newly updated internal TuRF implementation. \n\n\n\n\n\n\nFixed the pct paramter of TuRF to properly determine the percent of features removed each iteration as well as the total number of iterations as described in the original TuRF paper.  Also managed the edge case to ensure that at least one feature would be removed each TuRF iteration. \n\n\n\n\n\n\nFixed ability to parallelize run of core algorithm while using TuRF.\n\n\n\n\n\n\nUpdated the unit testing file to remove some excess unite tests, add other relevant ones, speed up testing overall, and make the testing better organized. \n\n\n\n\n\n\nAdded a preliminary implementation of VLSRelief to scikit-rebate, along with associated unit tests. Documentation and code examples not yet supported. \n\n\n\n\n\n\nRemoved some unused code from TuRF implementation.\n\n\n\n\n\n\nAdded check in the transform method required by scikit-learn in both relieff.py and turf.py to ensure that the number of selected features requested by the user was not larger than the number of features in the dataset. \n\n\n\n\n\n\nReduced the default value for number of features selected\n\n\n\n\n\n\nscikit-rebate 0.5\n\n\n\n\n\n\nAdded fixes to score normalizations that should ensure that feature scores for all algorithms fall between -1 and 1. \n\n\n\n\n\n\nAdded multi-class endpoint functionality. (now discriminates between binary and multiclass endpoints) Includes new methods for multi-class score update normalization.\n\n\n\n\n\n\nFixed normalization for missing data.\n\n\n\n\n\n\nFixed inconsistent pre-normalization for continuous feature data. \n\n\n\n\n\n\nAdded a custom ramp function to improve performance of all algorithms on data with a mix of discrete and continuous features.  Based on the standard deviation of a given continuous feature. \n\n\n\n\n\n\nUpdated the implementation of TuRF as an internal custom component of ReBATE.\n\n\n\n\n\n\nscikit-rebate 0.4\n\n\n\n\n\n\nAdded support for multicore processing to all Relief algorithms. Multiprocessing is now also supported in Python 2.\n\n\n\n\n\n\nThe \nReliefF\n algorithm now accepts float values in the range (0, 1.0] for the \nn_neighbors\n parameter. Float values will be interpreted as a fraction of the training set sample size.\n\n\n\n\n\n\nRefined the MultiSURF and MultiSURF* algorithms. From our internal research, MultiSURF is now one of our best-performing feature selection algorithms.\n\n\n\n\n\n\nscikit-rebate 0.3\n\n\n\n\n\n\nAdded a parallelization parameter, \nn_jobs\n, to ReliefF, SURF, SURF*, and MultiSURF via joblib.\n\n\n\n\n\n\nRenamed the \ndlimit\n parameter to \ndiscrete_limit\n to better reflect the purpose of the parameter.\n\n\n\n\n\n\nMinor code optimizations.\n\n\n\n\n\n\nscikit-rebate 0.2\n\n\n\n\n\n\nAdded documentation.\n\n\n\n\n\n\nMinor code optimizations.\n\n\n\n\n\n\nscikit-rebate 0.1\n\n\n\n\nInitial release of Relief algorithms, including ReliefF, SURF, SURF*, and MultiSURF.",
            "title": "Release Notes"
        },
        {
            "location": "/releases/#scikit-rebate-06",
            "text": "Fixed internal TuRF implementation so that it outputs scores for all features. Those that make it to the last iteration get true core algorithm scoring, while those that were removed along the way are assigned token scores (lower than the lowest true scoring feature) that indicate when the respective feature(s) were removed. This also alows for greater flexibility in the user specifying the number for features to return.     Updated the usage documentation to demonstrate how to use RFE as well as the newly updated internal TuRF implementation.     Fixed the pct paramter of TuRF to properly determine the percent of features removed each iteration as well as the total number of iterations as described in the original TuRF paper.  Also managed the edge case to ensure that at least one feature would be removed each TuRF iteration.     Fixed ability to parallelize run of core algorithm while using TuRF.    Updated the unit testing file to remove some excess unite tests, add other relevant ones, speed up testing overall, and make the testing better organized.     Added a preliminary implementation of VLSRelief to scikit-rebate, along with associated unit tests. Documentation and code examples not yet supported.     Removed some unused code from TuRF implementation.    Added check in the transform method required by scikit-learn in both relieff.py and turf.py to ensure that the number of selected features requested by the user was not larger than the number of features in the dataset.     Reduced the default value for number of features selected",
            "title": "scikit-rebate 0.6"
        },
        {
            "location": "/releases/#scikit-rebate-05",
            "text": "Added fixes to score normalizations that should ensure that feature scores for all algorithms fall between -1 and 1.     Added multi-class endpoint functionality. (now discriminates between binary and multiclass endpoints) Includes new methods for multi-class score update normalization.    Fixed normalization for missing data.    Fixed inconsistent pre-normalization for continuous feature data.     Added a custom ramp function to improve performance of all algorithms on data with a mix of discrete and continuous features.  Based on the standard deviation of a given continuous feature.     Updated the implementation of TuRF as an internal custom component of ReBATE.",
            "title": "scikit-rebate 0.5"
        },
        {
            "location": "/releases/#scikit-rebate-04",
            "text": "Added support for multicore processing to all Relief algorithms. Multiprocessing is now also supported in Python 2.    The  ReliefF  algorithm now accepts float values in the range (0, 1.0] for the  n_neighbors  parameter. Float values will be interpreted as a fraction of the training set sample size.    Refined the MultiSURF and MultiSURF* algorithms. From our internal research, MultiSURF is now one of our best-performing feature selection algorithms.",
            "title": "scikit-rebate 0.4"
        },
        {
            "location": "/releases/#scikit-rebate-03",
            "text": "Added a parallelization parameter,  n_jobs , to ReliefF, SURF, SURF*, and MultiSURF via joblib.    Renamed the  dlimit  parameter to  discrete_limit  to better reflect the purpose of the parameter.    Minor code optimizations.",
            "title": "scikit-rebate 0.3"
        },
        {
            "location": "/releases/#scikit-rebate-02",
            "text": "Added documentation.    Minor code optimizations.",
            "title": "scikit-rebate 0.2"
        },
        {
            "location": "/releases/#scikit-rebate-01",
            "text": "Initial release of Relief algorithms, including ReliefF, SURF, SURF*, and MultiSURF.",
            "title": "scikit-rebate 0.1"
        },
        {
            "location": "/citing/",
            "text": "If you use \nscikit-rebate\n or the \nMultiSURF\n algorithm in a scientific publication, please consider citing the following paper (currently available as a pre-print in arXiv):\n\n\nUrbanowicz, Ryan J., Randal S. Olson, Peter Schmitt, Melissa Meeker, and Jason H. Moore. \"Benchmarking relief-based feature selection methods.\" arXiv preprint arXiv:1711.08477 (2017).\n\n\nAlternatively a complete \nreview of Relief-based algorithms\n is available at:\n\n\nUrbanowicz, Ryan J., Melissa Meeker, William LaCava, Randal S. Olson, and Jason H. Moore. \"Relief-based feature selection: introduction and review.\" arXiv preprint arXiv:1711.08421 (2017).\n\n\nTo cite the \noriginal Relief\n paper:\n\n\nKira, Kenji, and Larry A. Rendell. \"A practical approach to feature selection.\" In Machine Learning Proceedings 1992, pp. 249-256. 1992.\n\n\nTo cite the \noriginal ReliefF\n paper: \n\n\nKononenko, Igor. \"Estimating attributes: analysis and extensions of RELIEF.\" In European conference on machine learning, pp. 171-182. Springer, Berlin, Heidelberg, 1994.\n\n\nTo cite the \noriginal SURF\n paper:\n\n\nGreene, Casey S., Nadia M. Penrod, Jeff Kiralis, and Jason H. Moore. \"Spatially uniform relieff (SURF) for computationally-efficient filtering of gene-gene interactions.\" BioData mining 2, no. 1 (2009): 5.\n\n\nTo cite the \noriginal SURF*\n paper: \n\n\nGreene, Casey S., Daniel S. Himmelstein, Jeff Kiralis, and Jason H. Moore. \"The informative extremes: using both nearest and farthest individuals can improve relief algorithms in the domain of human genetics.\" In European Conference on Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics, pp. 182-193. Springer, Berlin, Heidelberg, 2010.\n\n\nTo cite the \noriginal MultiSURF*\n paper:\n\n\nGranizo-Mackenzie, Delaney, and Jason H. Moore. \"Multiple threshold spatially uniform relieff for the genetic analysis of complex human diseases.\" In European Conference on Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics, pp. 1-10. Springer, Berlin, Heidelberg, 2013.\n\n\nTo cite the \noriginal TuRF\n paper: \n\n\nMoore, Jason H., and Bill C. White. \"Tuning ReliefF for genome-wide genetic analysis.\" In European Conference on Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics, pp. 166-175. Springer, Berlin, Heidelberg, 2007.",
            "title": "Citing"
        },
        {
            "location": "/support/",
            "text": "scikit-rebate was developed in the \nComputational Genetics Lab\n with funding from the \nNIH\n. We are incredibly grateful for their support during the development of this project.",
            "title": "Support"
        }
    ]
}