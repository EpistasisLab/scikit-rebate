<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Randal S. Olson and Ryan J. Urbanowicz">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Using skrebate - scikit-rebate</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Using skrebate";
    var mkdocs_page_input_path = "using.md";
    var mkdocs_page_url = "/using/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> scikit-rebate</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installing/">Installation</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Using skrebate</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#relieff">ReliefF</a></li>
    

    <li class="toctree-l2"><a href="#surf">SURF</a></li>
    

    <li class="toctree-l2"><a href="#surf_1">SURF*</a></li>
    

    <li class="toctree-l2"><a href="#multisurf">MultiSURF</a></li>
    

    <li class="toctree-l2"><a href="#multisurf_1">MultiSURF*</a></li>
    

    <li class="toctree-l2"><a href="#turf">TuRF</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#turf-implemented-in-scikit-rebate">TuRF implemented in scikit-rebate</a></li>
        
            <li><a class="toctree-l3" href="#turf-via-rfe">TuRF via RFE</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#acquiring-feature-importance-scores">Acquiring feature importance scores</a></li>
    

    <li class="toctree-l2"><a href="#general-usage-guidelines">General Usage Guidelines</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../contributing/">Contributing</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../releases/">Release Notes</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../citing/">Citing</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../support/">Support</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">scikit-rebate</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Using skrebate</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/EpistasisLab/scikit-rebate/edit/master/docs_sources/using.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>We have designed the Relief algorithms to be integrated directly into scikit-learn machine learning workflows. Below, we provide code samples showing how the various Relief algorithms can be used as feature selection methods in scikit-learn pipelines.</p>
<p>For details on the algorithmic differences between the various Relief algorithms, please refer to <a href="https://arxiv.org/abs/1711.08477">this research paper</a>.</p>
<h2 id="relieff">ReliefF</h2>
<p>ReliefF is the most basic of the Relief-based feature selection algorithms, and it requires you to specify the number of nearest neighbors to consider in the scoring algorithm. The parameters for the ReliefF algorithm are as follows:</p>
<table>
<tr>
<th>Parameter</th>
<th width="15%">Valid values</th>
<th>Effect</th>
</tr>
<tr>
<td>n_features_to_select</td>
<td>Any positive integer or float</td>
<td>The number of best features to retain after the feature selection process. The "best" features are the highest-scored features according to the ReliefF scoring process.</td>
</tr>
<tr>
<td>n_neighbors</td>
<td>Any positive integer</td>
<td>The number of neighbors to consider when assigning feature importance scores. If a float number is provided, that percentage of training samples is used as the number of neighbors. More neighbors results in more accurate scores, but takes longer.</td>
</tr>
<tr>
<td>discrete_limit</td>
<td>Any positive integer</td>
<td>Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.</td>
</tr>
<tr>
<td>n_jobs</td>
<td>Any positive integer or -1</td>
<td>The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.</td>
</tr>
</table>

<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import ReliefF
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="surf">SURF</h2>
<p>SURF, SURF*, and MultiSURF are all extensions to the ReliefF algorithm that automatically determine the ideal number of neighbors to consider when scoring the features.</p>
<table>
<tr>
<th>Parameter</th>
<th width="15%">Valid values</th>
<th>Effect</th>
</tr>
<tr>
<td>n_features_to_select</td>
<td>Any positive integer</td>
<td>The number of best features to retain after the feature selection process. The "best" features are the highest-scored features according to the SURF scoring process.</td>
</tr>
<tr>
<td>discrete_limit</td>
<td>Any positive integer</td>
<td>Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.</td>
</tr>
<tr>
<td>n_jobs</td>
<td>Any positive integer or -1</td>
<td>The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.</td>
</tr>
</table>

<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import SURF
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(SURF(n_features_to_select=2),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="surf_1">SURF*</h2>
<table>
<tr>
<th>Parameter</th>
<th width="15%">Valid values</th>
<th>Effect</th>
</tr>
<tr>
<td>n_features_to_select</td>
<td>Any positive integer</td>
<td>The number of best features to retain after the feature selection process. The "best" features are the highest-scored features according to the SURF* scoring process.</td>
</tr>
<tr>
<td>discrete_limit</td>
<td>Any positive integer</td>
<td>Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.</td>
</tr>
<tr>
<td>n_jobs</td>
<td>Any positive integer or -1</td>
<td>The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.</td>
</tr>
</table>

<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import SURFstar
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(SURFstar(n_features_to_select=2),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="multisurf">MultiSURF</h2>
<table>
<tr>
<th>Parameter</th>
<th width="15%">Valid values</th>
<th>Effect</th>
</tr>
<tr>
<td>n_features_to_select</td>
<td>Any positive integer</td>
<td>The number of best features to retain after the feature selection process. The "best" features are the highest-scored features according to the MultiSURF scoring process.</td>
</tr>
<tr>
<td>discrete_limit</td>
<td>Any positive integer</td>
<td>Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.</td>
</tr>
<tr>
<td>n_jobs</td>
<td>Any positive integer or -1</td>
<td>The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.</td>
</tr>
</table>

<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import MultiSURF
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(MultiSURF(n_features_to_select=2),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="multisurf_1">MultiSURF*</h2>
<table>
<tr>
<th>Parameter</th>
<th width="15%">Valid values</th>
<th>Effect</th>
</tr>
<tr>
<td>n_features_to_select</td>
<td>Any positive integer</td>
<td>The number of best features to retain after the feature selection process. The "best" features are the highest-scored features according to the MultiSURF* scoring process.</td>
</tr>
<tr>
<td>discrete_limit</td>
<td>Any positive integer</td>
<td>Value used to determine if a feature is discrete or continuous. If the number of unique levels in a feature is > discrete_threshold, then it is considered continuous, or discrete otherwise.</td>
</tr>
<tr>
<td>n_jobs</td>
<td>Any positive integer or -1</td>
<td>The number cores to dedicate to running the algorithm in parallel with joblib. Set to -1 to use all available cores.</td>
</tr>
</table>

<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import MultiSURFstar
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(MultiSURFstar(n_features_to_select=2),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="turf">TuRF</h2>
<p>TURF advances the feature selection process from a single round to a multi-round process, and can be used in conjunction with any of the Relief-based algorithms. TURF begins with all of the features in the first round, scores them using one of the Relief-based algorithms, then eliminates a portion of them that have the worst scores. With this reduced feature set, TURF again scores the remaining features and eliminates a portion of the worst-scoring features. This process is repeated until a predefined number of features remain or some maximum number of iterations have completed. Presently, there are two ways to run the 'TuRF' iterative feature selection wrapper around any of the given core Relief-based algorithm in scikit-rebate. First, there is a custom TuRF implementation, hard coded into scikit-rebate designed to operate in the same way as specified in the original TuRF paper.  The second, uses the <a href="http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination">Recursive Feature Elimination</a>, as <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html">implemented</a> in scikit-learn. These approaches are similar but not equivalent. We recommend using built in scikit-rebate TuRF. Examples for running TuRF using either approach are given below.</p>
<h3 id="turf-implemented-in-scikit-rebate">TuRF implemented in scikit-rebate</h3>
<p>With this TuRF implementation, the (pct) parameter inversely determines the number of TuRF scoring iterations (i.e 1/pct), and pct also determines the percent of features eliminated from scoring each iteration. The n_features_to_select parameter simply determines the number of top scoring features to pass onto the pipeline. This TuRF approach should be used to most closely follow the original TuRF description, as well as to be able to obtain individual feature scores following the completion of TuRF. This method also keeps information about when features were dropped from consideration during progressive TuRF iterations. It does this by assigning 'removed' features a token score that simply indicates which iteration the feature was removed from scoring. All features removed from scoring during TuRF will be assigned a score lower than the lowest feature score in the final feature set.  All features removed at the same time are assigned the same discounted token feature score. This is particularly important when accessing the feature scores as described later. For an example of how to use scikit-rebate TuRF in a scikit learn pipeline see below.</p>
<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import TuRF
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values
headers = list(genetic_data.drop(&quot;class&quot;, axis=1))

clf = make_pipeline(TuRF(core_algorithm=&quot;ReliefF&quot;, n_features_to_select=2, pct=0.5),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels, fit_params={'turf__headers': headers})))
&gt;&gt;&gt; 0.795
</code></pre>

<h3 id="turf-via-rfe">TuRF via RFE</h3>
<p>With this strategy for running TuRF the main difference is that the number of TuRF iterations is not controlled by the pct parameter, rather, iterations run until the specified number of n_features_to_select have been reached. Each iteration, the 'step' parameter controls the number of features removed, either as a percent between 0 and 1 or an integer count of features to remove each iteration. One critical shortcoming of this approach is that there is no way to obtain the individual feature scores when using RFE to do 'TuRF' scoring. See <a href="http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination">Recursive Feature Elimination</a>, for more details.</p>
<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import ReliefF
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

clf = make_pipeline(RFE(ReliefF(), n_features_to_select=2, step = 0.5),
                    RandomForestClassifier(n_estimators=100))

print(np.mean(cross_val_score(clf, features, labels)))
&gt;&gt;&gt; 0.795
</code></pre>

<h2 id="acquiring-feature-importance-scores">Acquiring feature importance scores</h2>
<p>In many cases, it may be useful to compute feature importance scores without actually performing feature selection. We have made it possible to access all Relief-based algorithm's scores via the <code>feature_importances_</code> attribute. Below are code examples showing how to access the scores from the any core Relief-based algorithm as well as from TuRF in combination with a Relief-based algorithm. The first example illustrates how scores may be obtained from ReliefF, adding a split of the loaded data into training and testing since we are not running ReliefF as part of a scikit pipeline like above.  </p>
<pre><code class="python">import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import ReliefF
from sklearn.model_selection import train_test_split

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values

# Make sure to compute the feature importance scores from only your training set
X_train, X_test, y_train, y_test = train_test_split(features, labels)

fs = ReliefF()
fs.fit(X_train, y_train)

for feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns,
                                       fs.feature_importances_):
    print(feature_name, '\t', feature_score)

&gt;&gt;&gt;N0    -0.0000166666666667
&gt;&gt;&gt;N1    -0.006175
&gt;&gt;&gt;N2    -0.0079
&gt;&gt;&gt;N3    -0.006275
&gt;&gt;&gt;N4    -0.00684166666667
&gt;&gt;&gt;N5    -0.0104416666667
&gt;&gt;&gt;N6    -0.010275
&gt;&gt;&gt;N7    -0.00785
&gt;&gt;&gt;N8    -0.00824166666667
&gt;&gt;&gt;N9    -0.00515
&gt;&gt;&gt;N10   -0.000216666666667
&gt;&gt;&gt;N11   -0.0039
&gt;&gt;&gt;N12   -0.00291666666667
&gt;&gt;&gt;N13   -0.00345833333333
&gt;&gt;&gt;N14   -0.00324166666667
&gt;&gt;&gt;N15   -0.00886666666667
&gt;&gt;&gt;N16   -0.00611666666667
&gt;&gt;&gt;N17   -0.007325
&gt;&gt;&gt;P1    0.108966666667
&gt;&gt;&gt;P2    0.111
</code></pre>

<p>In this second example we show how to obtain scores when using ReliefF in combination with TuRF (some slight differences). In this example we differently assume that the loaded dataset is the training dataset and we do not need to split the data into training and testing prior to running ReliefF. The main difference here is that when using TuRF, fs.fit also requires 'headers' as an argument.  </p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate.turf import TuRF
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'
                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',
                           sep='\t', compression='gzip')

features, labels = genetic_data.drop('class', axis=1).values, genetic_data['class'].values
headers = list(genetic_data.drop(&quot;class&quot;, axis=1))
fs = TuRF(core_algorithm=&quot;ReliefF&quot;, n_features_to_select=2, pct=0.5,verbose=True)
fs.fit(features, labels, headers)
for feature_name, feature_score in zip(genetic_data.drop('class', axis=1).columns, fs.feature_importances_):
    print(feature_name, '\t', feature_score)

&gt;&gt;&gt;N0    -0.00103125
&gt;&gt;&gt;N1    -0.0107515625
&gt;&gt;&gt;N2    -0.012890625
&gt;&gt;&gt;N3    -0.012890625
&gt;&gt;&gt;N4    -0.012890625
&gt;&gt;&gt;N5    -0.012890625
&gt;&gt;&gt;N6    -0.012890625
&gt;&gt;&gt;N7    -0.012890625
&gt;&gt;&gt;N8    -0.0107515625
&gt;&gt;&gt;N9    -0.012890625
&gt;&gt;&gt;N10   -0.00118125
&gt;&gt;&gt;N11   -0.012890625
&gt;&gt;&gt;N12   -0.0107515625
&gt;&gt;&gt;N13   -0.0086125
&gt;&gt;&gt;N14   -0.0107515625
&gt;&gt;&gt;N15   -0.012890625
&gt;&gt;&gt;N16   -0.0107515625
&gt;&gt;&gt;N17   -0.012890625
&gt;&gt;&gt;P1    0.20529375
&gt;&gt;&gt;P2    0.17374375
</code></pre>

<p>To retrieve an np.array of feature importance scores in the original dataset ordering then add the following... (this also works with any core algorithm)</p>
<pre><code>print(fs.feature\_importances\_)
&gt;&gt;&gt;[-0.00103125 -0.01075156 -0.01289062 -0.01289062 -0.01289062 -0.01289062
 -0.01289062 -0.01289062 -0.01075156 -0.01289062 -0.00118125 -0.01289062
 -0.01075156 -0.0086125  -0.01075156 -0.01289062 -0.01075156 -0.01289062
  0.20529375  0.17374375]
</code></pre>

<p>To retrieve a list of indices for the top scoring features ranked by increasing score, then add the following... (this also works with any core algorithm)</p>
<pre><code>print(fs.top\_features_)
&gt;&gt;&gt;[13, 0, 10, 19, 18]
</code></pre>

<p>To sort features by decreasing score along with their names, and simultaneously indicate which features have been assigned a token TuRF feature score (since they were removed from consideration at some point) then add the following...</p>
<pre><code>scored\_features = len(fs.top\_features_)
sorted_names = sorted(scoreDict, key=lambda x: scoreDict[x], reverse=True)
n = 1
for k in sorted\_names:
    if n &lt; scored\_features +1 :
        print(k, '\t', scoreDict[k],'\t',n) 
    else:
        print(k, '\t', scoreDict[k],'\t','*') 
    n += 1

&gt;&gt;&gt;P1    0.20529375      1
&gt;&gt;&gt;P2    0.17374375      2
&gt;&gt;&gt;N0    -0.00103125     3
&gt;&gt;&gt;N10   -0.00118125     4
&gt;&gt;&gt;N13   -0.0086125      5
&gt;&gt;&gt;N1    -0.0107515625   *
&gt;&gt;&gt;N14   -0.0107515625   *
&gt;&gt;&gt;N16   -0.0107515625   *
&gt;&gt;&gt;N8    -0.0107515625   *
&gt;&gt;&gt;N12   -0.0107515625   *
&gt;&gt;&gt;N3    -0.012890625    *
&gt;&gt;&gt;N2    -0.012890625    *
&gt;&gt;&gt;N7    -0.012890625    *
&gt;&gt;&gt;N17   -0.012890625    *
&gt;&gt;&gt;N5    -0.012890625    *
&gt;&gt;&gt;N15   -0.012890625    *
&gt;&gt;&gt;N11   -0.012890625    *
&gt;&gt;&gt;N4    -0.012890625    *
&gt;&gt;&gt;N9    -0.012890625    *
&gt;&gt;&gt;N6    -0.012890625    *
</code></pre>

<p>Lastly, to output these scores to a text file in a format similar to how it is done in our alternative implementation of stand alone <a href="https://github.com/EpistasisLab/ReBATE">ReBATE</a>, add something like the following...</p>
<pre><code>algorithm = 'TuRF_ReliefF'
discreteLimit = '10'
numNeighbors = '100'
outfile = algorithm + '-scores-' + discreteLimit + '-' + numNeighbors + '-' + 'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.txt'
fh = open(outfile, 'w')
fh.write(algorithm + ' Analysis Completed with REBATE\n')
fh.write('Run Time (sec): ' + str('NA') + '\n')
fh.write('=== SCORES ===\n')
n = 1
for k in sorted_names:
    if n &lt; scored_features +1 :
        fh.write(str(k) + '\t' + str(scoreDict[k])  + '\t' + str(n) +'\n')
    else:
        fh.write(str(k) + '\t' + str(scoreDict[k])  + '\t' + '*' +'\n')
    n+=1
fh.close()
</code></pre>

<p>This ordered list and text output can be achieved similarly for any core Relief-based algorithm by just removing the 'if n &lt; scored_features +1 :' loop and the else statement adding the '*'. </p>
<h2 id="general-usage-guidelines">General Usage Guidelines</h2>
<p>1.) When performing feature selection, there is no universally best way to determine where to draw the cuttoff for including features. When using original Relief or ReliefF it has been suggested that features yielding a negative value score, can be confidently filtered out. This guideline is believed to be extendable to SURF, SURF*, MultiSURF*, and MultiSURF, however please note that features with a negative score are not necessarily irrelevant, and those with a positive score are not necessarily relevant. Instead, scores are most effectively interpreted as the relative evidence that a given feature is predictive of outcome. Thus, while it may be reasonable to only filter out features with a negative score, in practice it may be more useful to select some 'top' number of features to pass onto modeling. </p>
<p>2.) In very large feature spaces users can expect core Relief-based algorithm scores to become less reliable when run on their own. This is because as the feature space becomes very large, the determination of nearest neighbors becomes more random.  As a result, in very large feature spaces (e.g. &gt; 10,000 features), users should consider combining a core Relief-based algorithm with an iterative approach such as TuRF (implemented here) or VLSRelieF, or Iterative Relief. </p>
<p>3.) When scaling up to big data problems, keep in mind that the data aspect that slows down ReBATE methods the most is the number of training instances, since Relief algorithms scale linearly with the number of features, but quadratically with the number of training instances. This is is the result of Relief-based methods needing to calculate a distance array (i.e. all pairwise distances between instances in the training dataset).  If you have a very large number of training instances available, consider utilizing a class balanced random sampling of that dataset when running any ReBATE methods to save on memory and computational time. </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../contributing/" class="btn btn-neutral float-right" title="Contributing">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../installing/" class="btn btn-neutral" title="Installation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Developed by <a href="http://www.randalolson.com">Randal S. Olson</a>, <a href="https://github.com/pschmitt52">Pete Schmitt</a>, and <a href="http://ryanurbanowicz.com">Ryan J. Urbanowicz</a> at the University of Pennsylvania</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/EpistasisLab/scikit-rebate/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../installing/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../contributing/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js"></script>
      <script src="../search/require.js"></script>
      <script src="../search/search.js"></script>

</body>
</html>
